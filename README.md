# Overview
Real-time Vision-based Sign Language to Text Interpreter by Using Artificial Intelligence and Augmented Reality Element by Muhammad Hafizuddin Rosnazri

Real-time Vision-based Sign Language to Text Interpreter by Using Artificial Intelligence and Augmented Reality Element is a project that can interpret sign language to text in real-time. This communicator used a machine learning approach with a slight touch of deep learning elements, which used OpenCV, MediaPipe, and Tensorflow algorithms. Those algorithms have been used to differentiate the hand from other objects, detect movement and produce Augmented Reality hand landmarks on the hand, and perform imagery data analysis to produce output in real-time. The camera will detect the user’s hand movement, and the output will be produced on the LCD monitor. This project has been developed using the Python programming language with Thonny Python IDE as the integrated development environment. 13,000 of ASL’s alphabets and 5,000 of ASL’s number imagery datasets have been trained by using a cloud platform, Google Colab. The training process for the alphabets produced 99.85% accuracy and 100% accuracy for the numbers. At the end of this project, the construction of a machine learning algorithm able to produce alphabets and numbers on LCD monitor in real-time and a workable prototype have been developed by using a USB camera, LCD monitor, and a Raspberry Pi microcontroller. The output in the form of text appeared on the LCD monitor by demonstrating ASL’s alphabet and number hand gestures. The performance of the prototype has been analyzed and experimented with by two users at plain and noise backgrounds with different determined distances. 
